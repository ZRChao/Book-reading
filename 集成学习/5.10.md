## 集成学习

在有监督学习中，单个学习器往往不能兼顾所有良好性质（弱监督模型），为此可以通过集结多个模型来提升总体效能。
**集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，
其他的弱分类器也可以将错误纠正回来。**,例如，随机森林。

#### bagging (bootstrap aggregating)

*在Bagging方法中，利用bootstrap方法从整体数据集中采取有放回抽样得到N个数据集，在每个数据集上学习出一个模型，
最后的预测结果利用N个模型的输出得到，具体地：分类问题采用N个模型预测投票的方式，回归问题采用N个模型预测平均的方式。*


#### Boosting

如；AdaBoost， Gradient Boost Decision Tree等

#### Stacking

首先训练出多个不同的模型，然后再以之前训练的各个模型的输出作为输入来新训练一个新的模型

### reference

https://zhuanlan.zhihu.com/p/27689464


推荐 ***** https://www.csdn.net/article/2015-03-02/2824069

**** https://vdeamov.github.io/机器学习/2018/08/24/集成学习/

https://blog.csdn.net/zwqjoy/article/details/80431496

http://www.datakit.cn/blog/2014/11/02/Ensemble_learning.html


### coding 

https://github.com/vsmolyakov/experiments_with_python/blob/master/chp01/ensemble_methods.ipynb
